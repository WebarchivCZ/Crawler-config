<?xml version="1.0" encoding="UTF-8"?>
<!-- 
  HERITRIX 3 CRAWL JOB CONFIGURATION FILE
  
   This is a relatively minimal configuration suitable for many crawls.
   
   Commented-out beans and properties are provided as an example; values
   shown in comments reflect the actual defaults which are in effect
   if not otherwise specified specification. (To change from the default 
   behavior, uncomment AND alter the shown values.)   
 -->
<beans xmlns="http://www.springframework.org/schema/beans"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:context="http://www.springframework.org/schema/context"
        xmlns:aop="http://www.springframework.org/schema/aop"
        xmlns:tx="http://www.springframework.org/schema/tx"
        xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
           http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
           http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd
           http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd">
 
 <context:annotation-config/>

<!-- 
  OVERRIDES
   Values elsewhere in the configuration may be replaced ('overridden') 
   by a Properties map declared in a PropertiesOverrideConfigurer, 
   using a dotted-bean-path to address individual bean properties. 
   This allows us to collect a few of the most-often changed values
   in an easy-to-edit format here at the beginning of the model
   configuration.    
 -->
 <!-- overrides from a text property list -->
 <bean id="simpleOverrides" class="org.springframework.beans.factory.config.PropertyOverrideConfigurer">
  <property name="properties">
   <value>
# This Properties map is specified in the Java 'property list' text format
# http://java.sun.com/javase/6/docs/api/java/util/Properties.html#load%28java.io.Reader%29

# Values changing with each crawl
metadata.jobName=Krajské a senátní volby 2016
metadata.description=Tématická sklizeň ke krajským a senátním volbám 2016
warcWriter.prefix=KrajskeVolby2016-04
warcWriter.storePaths=/mnt/archives/14/2016/topics/KrajskeVolby2016
frontier.queueTotalBudget=-1
# INSPECT: There is change that balanceReplenishAmount doesnt work as expected IE: Crawl 5k URIs, and wait for other seeds to crawl until 5k URIs - check FRONTIER
frontier.balanceReplenishAmount=5000
# YOUTUBE: Do you wanna save youtube videos?
extractorYoutubeWatchPage.enabled=true

# bdb for Heritrix states
bdb.dir=/opt/heritrix/jobs/bdb-state-KrajskeVolby2016

# URL-Agnostic Duplication Reduction - https://webarchive.jira.com/wiki/display/Heritrix/Duplication+Reduction+Processors
# State directory for duplication reduction. Use for topics, which will crawled multipletimes.

historyBdb.dir=/opt/heritrix/jobs/bdb-state-history-KrajskeVolby2016

# DISTRIBUTED CRAWLING - if true, check if proper localName is set per Crawler and do not forget to mention how many crawlers will join the party. -->
hashCrawlMapper.enabled=false
# Local name of Crawler is number starting with 0.
hashCrawlMapper.localName=0
# Number of crawlers among which to split up the URIs.
hashCrawlMapper.crawlerCount=3
# Diversions directory should be shared between crawlers per job.
hashCrawlMapper.diversionDir=/mnt/archives/archive12/pre-fetch-diversions

# FACEBOOK / TWITTER crawling
# INSPECT: I am not sure how to properly override sheets, so if you wanna turn off FB/TWITTER extraction, set extractorFacebookScroll.enabled and extractorTwitterScrollOne.enabled and extractorTwitterScrollFurther.enabled to false.

# Stable values
metadata.operatorContactUrl=http://webarchiv.cz/kontakty/
metadata.operator=Rudolf Kreibich
metadata.operatorFrom=webarchiv@nkp.cz
metadata.organization=National Library of the Czech Republic - Webarchiv.cz
metadata.audience=Webarchiv.cz Users
metadata.userAgentTemplate=Mozilla/5.0 (compatible; heritrix/@VERSION@ +@OPERATOR_CONTACT_URL@)
metadata.robotsPolicyName=ignore

##..more?..##
   </value>
  </property>
 </bean>

 <!-- overrides from declared <prop> elements, more easily allowing
      multiline values or even declared beans -->
<!--
 <bean id="longerOverrides" class="org.springframework.beans.factory.config.PropertyOverrideConfigurer">
  <property name="properties">
   <props>
    <prop key="seeds.textSource.value">

# URLS HERE
http://example.example/example

    </prop>
   </props>
  </property>
 </bean>
-->

 <!-- CRAWL METADATA: including identification of crawler/operator -->
 <bean id="metadata" class="org.archive.modules.CrawlMetadata" autowire="byName">
       <property name="operatorContactUrl" value="[see override above]"/>
       <property name="jobName" value="[see override above]"/>
       <property name="description" value="[see override above]"/>
       <property name="robotsPolicyName" value="[see override above]"/>
       <property name="operator" value="[see override above]"/>
       <property name="operatorFrom" value="[see override above]"/>
       <property name="organization" value="[see override above]"/>
       <property name="audience" value="[see override above]"/>
       <property name="userAgentTemplate" value="[see override above]"/>
 </bean>
 
 <!-- SEEDS: crawl starting points 
      ConfigString allows simple, inline specification of a moderate
      number of seeds; see below comment for example of using an
      arbitrarily-large external file. -->
<!--
 <bean id="seeds" class="org.archive.modules.seeds.TextSeedModule">
     <property name="textSource">
      <bean class="org.archive.spring.ConfigString">
       <property name="value">
        <value>
# [see override above]
        </value>
       </property>
      </bean>
     </property>
-->
<!-- <property name='sourceTagSeeds' value='false'/> -->
<!-- <property name='blockAwaitingSeedLines' value='-1'/> -->
<!--
 </bean>
-->

 <!-- SEEDS ALTERNATE APPROACH: specifying external seeds.txt file in
      the job directory, similar to the H1 approach. 
      Use either the above, or this, but not both. -->
 <bean id="seeds" class="org.archive.modules.seeds.TextSeedModule">
  <property name="textSource">
   <bean class="org.archive.spring.ConfigFile">
    <property name="path" value="seeds.txt" />
   </bean>
  </property>
  <!-- sourceTagSeeds: Whether to tag seeds with their own URI as a heritable 'source' String, which will be carried-forward to all URIs discovered on paths originating from that seed. When present, such source tags appear in the second-to-last crawl.log field. https://webarchive.jira.com/wiki/display/Heritrix/AbstractFrontier+source-tag-seeds -->
  <property name='sourceTagSeeds' value='true'/>
  <!-- Number of lines of seeds-source to read on initial load before proceeding with crawl. Default is -1, meaning all. Any other value will cause that number of lines to be loaded before fetching begins, while all extra lines continue to be processed in the background. Generally, this should only be changed when working with very large seed lists, and scopes that do *not* depend on reading all seeds. http://builds.archive.org:8080/javadoc/heritrix-3.1.0/org/archive/modules/seeds/TextSeedModule.html#blockAwaitingSeedLines -->
  <property name='blockAwaitingSeedLines' value='-1'/>
 </bean>
 
 <bean id="acceptSurts" class="org.archive.modules.deciderules.surt.SurtPrefixedDecideRule">
  <!-- <property name="decision" value="ACCEPT"/> -->
  <!-- <property name="seedsAsSurtPrefixes" value="true" /> -->
  <!-- <property name="alsoCheckVia" value="false" /> -->
  <!-- <property name="surtsSourceFile" value="" /> -->
  <property name="surtsDumpFile" value="${launchId}/surts.dump" />
  <property name="surtsSource">
   <bean class="org.archive.spring.ConfigFile">
    <property name="path" value="../Shared-config/surts.txt" />
   </bean>
  </property>
 </bean>

 <!-- SCOPE: rules for which discovered URIs to crawl; order is very 
      important because last decision returned other than 'NONE' wins. -->
 <bean id="scope" class="org.archive.modules.deciderules.DecideRuleSequence">
  <!-- <property name="logToFile" value="false" /> -->
  <property name="rules">
   <list>
    <!-- Begin by REJECTing all... -->
    <bean class="org.archive.modules.deciderules.RejectDecideRule" />
    <!-- ...then ACCEPT those within configured/seed-implied SURT prefixes... -->
    <ref bean="acceptSurts" />
    <!-- ...but REJECT those more than a configured link-hop-count from start... -->
    <bean class="org.archive.modules.deciderules.TooManyHopsDecideRule">
     <!-- <property name="maxHops" value="20" /> -->
    </bean>
    <!-- ...but ACCEPT those more than a configured link-hop-count from start... -->
    <bean class="org.archive.modules.deciderules.TransclusionDecideRule">
     <!-- <property name="maxTransHops" value="2" /> -->
     <!-- <property name="maxSpeculativeHops" value="1" /> -->
    </bean>
    <!-- ...but REJECT those from a configurable (initially empty) set of REJECT SURTs... -->
    <bean class="org.archive.modules.deciderules.surt.SurtPrefixedDecideRule">
          <property name="decision" value="REJECT"/>
          <property name="seedsAsSurtPrefixes" value="false"/>
          <property name="surtsDumpFile" value="${launchId}/negative-surts.dump" /> 
          <property name="surtsSource">
           <bean class="org.archive.spring.ConfigFile">
            <property name="path" value="../Shared-config/negative-surts.txt" />
           </bean>
          </property>
    </bean>
    <!-- ...and REJECT those from a configurable (initially empty) set of URI regexes... -->
    <bean class="org.archive.modules.deciderules.MatchesListRegexDecideRule">
          <property name="decision" value="REJECT"/>
     <!-- <property name="listLogicalOr" value="true" /> -->
     <!-- INSPECT: Can i make Regex list external? -->
	  <property name="regexList">
           <list>
<!-- Disabling global exclusion
                <value>.*/.*wordpress3\.0\.1$</value>
                <value>.*//www\.tai2\.cz.$</value>
                <value>.*/AddComment\.asp.*</value>
                <value>.*/\d{1,2}\.\d{1,2}\.\d{4}$</value>
                <value>.*/ajax/hodnoceni-clanek/.*</value>
                <value>.*/atom_cs$</value>
                <value>.*/captcha\.php.*</value>
                <value>.*/clanek/preposlat/.*</value>
                <value>.*/commentAction/.*</value>
                <value>.*/comment\.php\?akce=new.*</value>
                <value>.*/ecard\.php.*</value>
                <value>.*/edit\.php.*</value>
                <value>.*/feed$</value>
                <value>.*/feed/.*</value>
                <value>.*/feeds/.*</value>
                <value>.*/fullTextSearch\.asp.*</value>
                <value>.*/hlasuj\.asp.*</value>
                <value>.*/like\.php.*</value>
                <value>.*/login/.*</value>
                <value>.*/login\.asp.*</value>
                <value>.*/login\.php.*</value>
                <value>.*/mailto/.*</value>
                <value>.*/odeslat-odkaz-e-mailem\?.*</value>
                <value>.*/rss$</value>
                <value>.*/rss/.*</value>
                <value>.*/send_link\.php.*</value>
                <value>.*/share.?\.php.*</value>
                <value>.*/wp-login\.php.*</value>
                <value>.*ZaslatEmailem.*</value>
                <value>.*\?add_disc.*</value>
                <value>.*\?album=random.*</value>
                <value>.*\?like=1.*</value>
                <value>.*\?sel_ids=1$</value>
                <value>.*a\.lightbox.*</value>
                <value>.*adlog\.*php.*</value>
                <value>.*akce=comment_re.*</value>
                <value>.*cid=AuthCookie.*</value>
                <value>.*format=feed.*</value>
                <value>.*forwardAuthenticated\.jsp.*</value>
                <value>.*function\.fopen.*</value>
                <value>.*google\.com/bookmarks/mark\?.*</value>
                <value>.*l_op=brokenlink.*</value>
                <value>.*l_op=ratelink.*</value>
                <value>.*name=Your_Account.*</value>
                <value>.*op=FriendSend.*</value>
                <value>.*replytocom=.*</value>
                <value>.*sps.cz/RDS/type=news.*</value>
                <value>.*ul.enhanced.*</value>
                <value>http://twitter\.com/login.*</value>
                <value>http://twitter\.com/share.*</value>
                <value>http://www\.google\.com/reader.*</value>
                <value>www.df.biom.cz.*</value>
                <value>www.geofond.cz/mapsphere/EEARTH/default.aspx?lang=cs.*</value>
                <value>www.geofond.cz/mapsphere/MapWin.aspx?M_WizID=24&amp;M_Site=geofond&amp;M_Lang=cs.*</value>
                <value>www.spnv.cz/file.php?nid=9560&amp;oid=1998061.*</value>
                <value>www.spnv.cz/file.php?nid=9560&amp;oid=1998070.*</value>
                <value>www.svaz.chf.cz/shop.aspx?catid=225.*</value>
-->
           </list>
          </property>
    </bean>
    <!-- ...and REJECT those with suspicious repeating path-segments... -->
    <bean class="org.archive.modules.deciderules.PathologicalPathDecideRule">
     <!-- <property name="maxRepetitions" value="2" /> -->
    </bean>
    <!-- ...and REJECT those with more than threshold number of path-segments... -->
    <bean class="org.archive.modules.deciderules.TooManyPathSegmentsDecideRule">
     <!-- <property name="maxPathDepth" value="20" /> -->
    </bean>
    <!-- ...but always ACCEPT those marked as prerequisitee for another URI... -->
    <bean class="org.archive.modules.deciderules.PrerequisiteAcceptDecideRule">
    </bean>
    <!-- ...but always REJECT those with unsupported URI schemes -->
    <bean class="org.archive.modules.deciderules.SchemeNotInSetDecideRule">
    </bean>
   </list>
  </property>
 </bean>
 
 <!-- 
   PROCESSING CHAINS
    Much of the crawler's work is specified by the sequential 
    application of swappable Processor modules. These Processors
    are collected into three 'chains'. The CandidateChain is applied 
    to URIs being considered for inclusion, before a URI is enqueued
    for collection. The FetchChain is applied to URIs when their 
    turn for collection comes up. The DispositionChain is applied 
    after a URI is fetched and analyzed/link-extracted.
  -->

<!-- DISTRIBUTED CRAWLING -->	
 <bean id="hashCrawlMapper" class="org.archive.crawler.processor.HashCrawlMapper">
  <property name="enabled" value="[see override above]" />
  <property name="localName" value="[see override above]" />
  <property name="diversionDir" value="[see override above]" />
  <property name="checkUri" value="true" />
  <property name="checkOutlinks" value="false"/>
  <property name="rotationDigits" value="10" />
  <property name="crawlerCount" value="[see override above]" />
</bean>

 <!-- CANDIDATE CHAIN --> 
 <!-- first, processors are declared as top-level named beans -->
 <bean id="candidateScoper" class="org.archive.crawler.prefetch.CandidateScoper">
 </bean>
 <bean id="preparer" class="org.archive.crawler.prefetch.FrontierPreparer">
  <!-- <property name="preferenceDepthHops" value="-1" /> -->
  <!-- <property name="preferenceEmbedHops" value="1" /> -->
  <!-- <property name="canonicalizationPolicy"> 
        <ref bean="canonicalizationPolicy" />
       </property> -->
  <!-- <property name="queueAssignmentPolicy"> 
        <ref bean="queueAssignmentPolicy" />
       </property> -->
  <!-- <property name="uriPrecedencePolicy"> 
        <ref bean="uriPrecedencePolicy" />
       </property> -->
  <!-- <property name="costAssignmentPolicy"> 
        <ref bean="costAssignmentPolicy" />
       </property> -->
 </bean>
 <!-- now, processors are assembled into ordered CandidateChain bean -->
 <bean id="candidateProcessors" class="org.archive.modules.CandidateChain">
  <property name="processors">
   <list>
    <!-- apply scoping rules to each individual candidate URI... -->
    <ref bean="candidateScoper"/>
    <!-- DISTRIBUTED CRAWLING -->
    <ref bean="hashCrawlMapper"/>
    <!-- ...then prepare those ACCEPTed to be enqueued to frontier. -->
    <ref bean="preparer"/>
   </list>
  </property>
 </bean>
  
 <!-- FETCH CHAIN --> 
 <!-- first, processors are declared as top-level named beans -->
 <bean id="preselector" class="org.archive.crawler.prefetch.Preselector">
  <!-- <property name="recheckScope" value="false" /> -->
  <!-- <property name="blockAll" value="false" /> -->
  <!-- <property name="blockByRegex" value="" /> -->
  <!-- <property name="allowByRegex" value="" /> -->
 </bean>
 <bean id="preconditions" class="org.archive.crawler.prefetch.PreconditionEnforcer">
  <!-- <property name="ipValidityDurationSeconds" value="21600" /> -->
  <!-- <property name="robotsValidityDurationSeconds" value="86400" /> -->
  <!-- <property name="calculateRobotsOnly" value="false" /> -->
 </bean>
 <bean id="fetchDns" class="org.archive.modules.fetcher.FetchDNS">
  <!-- <property name="acceptNonDnsResolves" value="false" /> -->
  <!-- <property name="digestContent" value="true" /> -->
  <!-- <property name="digestAlgorithm" value="sha1" /> -->
 </bean>
 <!-- <bean id="fetchWhois" class="org.archive.modules.fetcher.FetchWhois">
       <property name="specialQueryTemplates">
        <map>
         <entry key="whois.verisign-grs.com" value="domain %s" />
         <entry key="whois.arin.net" value="z + %s" />
         <entry key="whois.denic.de" value="-T dn %s" />
        </map>
       </property> 
      </bean> -->
 <bean id="fetchHttp" class="org.archive.modules.fetcher.FetchHTTP">
  <!-- <property name="useHTTP11" value="false" /> -->
  <!-- <property name="maxLengthBytes" value="0" /> -->
  <!-- <property name="timeoutSeconds" value="1200" /> -->
  <!-- <property name="maxFetchKBSec" value="0" /> -->
  <!-- <property name="defaultEncoding" value="ISO-8859-1" /> -->
  <!-- <property name="shouldFetchBodyRule"> 
        <bean class="org.archive.modules.deciderules.AcceptDecideRule"/>
       </property> -->
  <!-- <property name="soTimeoutMs" value="20000" /> -->
<!-- DUPLICATION REDUCTION -->
<!-- In the FetchHTTP processor, the sendIfNoneMatch and sendIfModifiedSince properties control whether the HTTP If-None-Match or If-Modified-Since headers are sent on a request if the URI history information (prior-fetch date or etag info) to support them is present.
Note that by sending these headers, the crawler may receive a '304-Not Modified' response from the server, with no body content. In this way, other URIs may not be discovered, and paths followed by the original crawl not reconsidered for refetching. Thus if you leave sendIfNoneMatch and sendIfModifiedSince set as their default 'true' values on a deduplication-history-enabled crawl, you may wish to use some other technique to ensure all URIs from the earlier crawls are reconsidered (such as feeding them all to the crawl at the start).
-->  
  <property name="sendIfModifiedSince" value="false" />
  <property name="sendIfNoneMatch" value="false" />
  <!-- <property name="sendConnectionClose" value="true" /> -->
  <!-- <property name="sendReferer" value="true" /> -->
  <!-- <property name="sendRange" value="false" /> -->
  <!-- <property name="ignoreCookies" value="false" /> -->
  <!-- <property name="sslTrustLevel" value="OPEN" /> -->
  <!-- <property name="acceptHeaders"> 
        <list>
         <value>Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8</value>
        </list>
       </property>
  -->
  <!-- <property name="httpBindAddress" value="" /> -->
  <!-- <property name="httpProxyHost" value="" /> -->
  <!-- <property name="httpProxyPort" value="0" /> -->
  <!-- <property name="httpProxyUser" value="" /> -->
  <!-- <property name="httpProxyPassword" value="" /> -->
  <!-- <property name="digestContent" value="true" /> -->
  <!-- <property name="digestAlgorithm" value="sha1" /> -->
 </bean>
 <bean id="extractorHttp" class="org.archive.modules.extractor.ExtractorHTTP"/>
 <bean id="extractorHtml" class="org.archive.modules.extractor.ExtractorHTML">
  <!-- <property name="extractJavascript" value="true" /> -->
  <!-- <property name="extractValueAttributes" value="true" /> -->
  <!-- <property name="ignoreFormActionUrls" value="false" /> -->
  <!-- <property name="extractOnlyFormGets" value="true" /> -->
  <!-- <property name="treatFramesAsEmbedLinks" value="true" /> -->
  <!-- <property name="ignoreUnexpectedHtml" value="true" /> -->
  <!-- <property name="maxElementLength" value="1024" /> -->
  <!-- <property name="maxAttributeNameLength" value="1024" /> -->
  <!-- <property name="maxAttributeValueLength" value="16384" /> -->
 </bean>
 <bean id="extractorCss" class="org.archive.modules.extractor.ExtractorCSS"/>
 <bean id="extractorJs" class="org.archive.modules.extractor.ExtractorJS"/>
 <bean id="extractorXml" class="org.archive.modules.extractor.ExtractorXML"/>
 <bean id="extractorSwf" class="org.archive.modules.extractor.ExtractorSWF"/>

<!--
#
# CUSTOM EXTRACTORS:
#
-->

<!-- YOUTUBE EXTRACTION - thx to Noah -->
 <bean id="extractorYoutubeEmbedded" class="org.archive.modules.extractor.ExtractorImpliedURI">
  <property name="regex" value="^(http://(?:www.)?youtube.com)/(?:v|embed)/([a-zA-Z0-9_-]+).*$"/>
  <property name="format" value="$1/watch?v=$2"/>
 </bean>
 <bean id="extractorYoutubeWatchPage" class="org.archive.modules.extractor.ExtractorImpliedURI">
  <property name="regex" value="^(http://[^/]*\.c\.youtube\.com)/[^?]+\?(.*)$"/>
  <property name="format" value="$1/videoplayback?$2"/>
  <property name="enabled" value="[see override above]"/>
 </bean>


<!-- FACEBOOK Extraction - https://webarchive.jira.com/wiki/display/Heritrix/Facebook+and+Twitter+Scroll-down -->
 <bean id="extractorFacebookScroll" class="org.archive.modules.extractor.ExtractorMultipleRegex">
  <property name="enabled" value="false"/>
  <property name="uriRegex" value="^https?://(?:www\.)?facebook\.com/[^/?]+$"/>
  <property name="contentRegexes">
   <map>
    <entry key="jsonBlob" value="\[&quot;TimelineContentLoader&quot;,&quot;registerTimePeriod&quot;,[^,]+,[^,]+,[^,]+,\{(&quot;profile_id&quot;:[^}]+)\},false,null,(\d+),"/>
    <entry key="ajaxpipeToken" value="&quot;ajaxpipe_token&quot;:&quot;([^&quot;]+)&quot;"/>
    <entry key="timeCutoff" value="&quot;setTimeCutoff&quot;,[^,]*,\[(\d+)\]\]"/>
   </map>
  </property>
  <property name="template">
    <value>/ajax/pagelet/generic.php/ProfileTimelineSectionPagelet?ajaxpipe=1&amp;ajaxpipe_token=${ajaxpipeToken[1]}&amp;no_script_path=1&amp;data=${java.net.URLEncoder.encode('{' + jsonBlob[1] , 'UTF-8')},&quot;time_cutoff&quot;%3A${java.net.URLEncoder.encode(timeCutoff[1] , 'UTF-8')},&quot;force_no_friend_activity&quot;%3Afalse%7D&amp;__user=0&amp;__a=1&amp;__adt=${jsonBlob[2]}</value>
  </property>
 </bean>

<!-- TWITTER Extraction - https://webarchive.jira.com/wiki/display/Heritrix/Facebook+and+Twitter+Scroll-down -->
 <bean id="extractorTwitterScrollOne" class="org.archive.modules.extractor.ExtractorMultipleRegex">
  <property name="enabled" value="false"/>
  <property name="uriRegex" value="^https?://(?:www\.)?twitter\.com/([^/]+)/?(?:\?.*)?$"/>
  <property name="contentRegexes">
   <map>
    <entry key="maxId" value="data-max-id=&quot;(\d+)&quot;"/>
   </map>
  </property>
  <property name="template">
   <value>/i/profiles/show/${uriRegex[1]}/timeline/with_replies?include_available_features=1&amp;include_entities=1&amp;max_id=${maxId[1]}</value>
  </property>
 </bean>
 <bean id="extractorTwitterScrollFurther" class="org.archive.modules.extractor.ExtractorMultipleRegex">
  <property name="enabled" value="false"/>
  <property name="uriRegex" value="^https?://(?:www\.)?twitter\.com/i/profiles/show/([^/]+)/timeline/with_replies\?include_available_features=1&amp;include_entities=1&amp;max_id=\d+$"/>
  <property name="contentRegexes">
   <map>
    <entry key="maxId" value="&quot;max_id&quot;:&quot;(\d+)&quot;"/>
   </map>
  </property>
  <property name="template">
   <value>/i/profiles/show/${uriRegex[1]}/timeline/with_replies?include_available_features=1&amp;include_entities=1&amp;max_id=${maxId[1]}</value>
  </property>
 </bean>

    
 <!-- now, processors are assembled into ordered FetchChain bean -->
 <bean id="fetchProcessors" class="org.archive.modules.FetchChain">
  <property name="processors">
   <list>
    <!-- re-check scope, if so enabled... -->
    <ref bean="preselector"/>
    <!-- ...then verify or trigger prerequisite URIs fetched, allow crawling... -->
    <!-- DISTRIBUTED CRAWLING, INSPECT! hashCrawlMapperProcessor should be here as stated in yahoo groups, bud i found no bean for it so Heritrix complains and i am changing it for now to hashCrawlMapper:-->
    <ref bean="hashCrawlMapper"/>
    <ref bean="preconditions"/>
    <!-- ...fetch if DNS URI... -->
    <ref bean="fetchDns"/>
    <!-- <ref bean="fetchWhois"/> -->
    <!-- ...fetch if HTTP URI... -->
    <ref bean="fetchHttp"/>
<!-- ...extract outlinks from HTTP headers... -->
    <ref bean="extractorHttp"/>
    <!-- ...extract outlinks from HTML content... -->
    <ref bean="extractorHtml"/>
    <!-- ...extract outlinks from CSS content... -->
    <ref bean="extractorCss"/>
    <!-- ...extract outlinks from Javascript content... -->
    <ref bean="extractorJs"/>
    <!-- ...extract outlinks from Flash content... -->
    <ref bean="extractorSwf"/>
<!-- CUSTOM EXTRACTORS -->
    <ref bean="extractorFacebookScroll"/>
    <ref bean="extractorTwitterScrollOne"/>
    <ref bean="extractorTwitterScrollFurther"/>
    <ref bean="extractorYoutubeWatchPage"/>
   </list>
  </property>
 </bean>
  
 <!-- DISPOSITION CHAIN -->
 <!-- first, processors are declared as top-level named beans  -->
 <bean id="warcWriter" class="org.archive.modules.writer.WARCWriterProcessor">
  <!-- <property name="compress" value="true" /> -->
  <property name="prefix" value="[See override above]"/>
  <!-- <property name="suffix" value="${HOSTNAME}" /> -->
  <!-- <property name="maxFileSizeBytes" value="1000000000" /> -->
  <!-- INSPECT: in H1 we had 5 arc pool, setting to 2 for now -->
       <property name="poolMaxActive" value="2" />
  <!-- <property name="MaxWaitForIdleMs" value="500" /> -->
  <!-- <property name="skipIdenticalDigests" value="false" /> -->
  <!-- <property name="maxTotalBytesToWrite" value="0" /> -->
  <!-- <property name="directory" value="${launchId}" /> -->
  <property name="storePaths">
	<list>
         <value>"[See override above]"</value>
	</list>
       </property>
  <!-- <property name="writeRequests" value="true" /> -->
  <!-- <property name="writeMetadata" value="true" /> -->
  <!-- <property name="writeRevisitForIdenticalDigests" value="true" /> -->
  <!-- <property name="writeRevisitForNotModified" value="true" /> -->
 </bean>
 <bean id="candidates" class="org.archive.crawler.postprocessor.CandidatesProcessor">
  <!-- <property name="seedsRedirectNewSeeds" value="true" /> -->
  <!-- <property name="processErrorOutlinks" value="false" /> -->
 </bean>
 <bean id="disposition" class="org.archive.crawler.postprocessor.DispositionProcessor">
  <!-- <property name="delayFactor" value="5.0" /> -->
  <!-- <property name="minDelayMs" value="3000" /> -->
  <!-- <property name="respectCrawlDelayUpToSeconds" value="300" /> -->
  <!-- <property name="maxDelayMs" value="30000" /> -->
  <!-- <property name="maxPerHostBandwidthUsageKbSec" value="0" /> -->
 </bean>
 <!-- <bean id="rescheduler" class="org.archive.crawler.postprocessor.ReschedulingProcessor">
       <property name="rescheduleDelaySeconds" value="-1" />
      </bean> -->
 <!-- now, processors are assembled into ordered DispositionChain bean -->
 <bean id="dispositionProcessors" class="org.archive.modules.DispositionChain">
  <property name="processors">
   <list>
    <!-- URL-Agnostic Duplication Reduction -->
    <bean class="org.archive.modules.recrawl.ContentDigestHistoryLoader" />
    <!-- write to aggregate archival files... -->
    <ref bean="warcWriter"/>
    <!-- URL-Agnostic Duplication Reduction -->
    <bean class="org.archive.modules.recrawl.ContentDigestHistoryStorer" />
    <!-- ...send each outlink candidate URI to CandidateChain, 
         and enqueue those ACCEPTed to the frontier... -->
    <ref bean="candidates"/>
    <!-- ...then update stats, shared-structures, frontier decisions -->
    <ref bean="disposition"/>
    <!-- <ref bean="rescheduler" /> -->
   </list>
  </property>
 </bean>
 
 <!-- CRAWLCONTROLLER: Control interface, unifying context -->
 <bean id="crawlController" 
   class="org.archive.crawler.framework.CrawlController">
	<property name="maxToeThreads" value="200" />
  <!-- <property name="pauseAtStart" value="true" /> -->
  <!-- <property name="runWhileEmpty" value="false" /> -->
  <!-- <property name="recorderInBufferBytes" value="524288" /> -->
  <!-- <property name="recorderOutBufferBytes" value="16384" /> -->
  <!-- <property name="scratchDir" value="scratch" /> -->
 </bean>
 
 <!-- FRONTIER: Record of all URIs discovered and queued-for-collection -->
 <bean id="frontier" 
   class="org.archive.crawler.frontier.BdbFrontier">
  <property name="queueTotalBudget" value="[see override above]" />
  <property name="balanceReplenishAmount" value="[see override above]" />
  <!-- <property name="errorPenaltyAmount" value="100" /> -->
  <!-- <property name="precedenceFloor" value="255" /> -->
<!-- INSPECT! Does BaseQueuePrecedencePolicy works with balanceReplenishAmount? -->
  <!-- <property name="queuePrecedencePolicy">
        <bean class="org.archive.crawler.frontier.precedence.BaseQueuePrecedencePolicy" />
       </property> -->
  <!-- <property name="snoozeLongMs" value="300000" /> -->
  <!-- <property name="retryDelaySeconds" value="900" /> -->
  <!-- <property name="maxRetries" value="30" /> -->
  <!-- <property name="recoveryLogEnabled" value="true" /> -->
  <!-- <property name="maxOutlinks" value="6000" /> -->
  <!-- <property name="extractIndependently" value="false" /> -->
  <!-- <property name="outbound">
        <bean class="java.util.concurrent.ArrayBlockingQueue">
         <constructor-arg value="200"/>
         <constructor-arg value="true"/>
        </bean>
       </property> -->
  <!-- <property name="inbound">
        <bean class="java.util.concurrent.ArrayBlockingQueue">
         <constructor-arg value="40000"/>
         <constructor-arg value="true"/>
        </bean>
       </property> -->
  <!-- <property name="dumpPendingAtClose" value="false" /> -->
 </bean>
 
 <!-- URI UNIQ FILTER: Used by frontier to remember already-included URIs --> 
 <bean id="uriUniqFilter" 
   class="org.archive.crawler.util.BdbUriUniqFilter">
 </bean>

<!--
   CUSTOM OVERLAY SHEETS
-->

<!-- LOCAL REGEX ASSOCIATION-->

<!-- LOCAL CALENDARS -->
 <bean id="rejectLocalCalendars" class="org.archive.modules.deciderules.MatchesListRegexDecideRule" autowire-candidate="false">
  <property name="decision" value="REJECT"/>
 </bean>
 <bean id="rejectLocalCalendars-sheet" class="org.archive.spring.Sheet">
  <property name="map">
   <map>
    <entry key="rejectLocalCalendars.regexList">
     <list>
<!--TODO REFINE EXPRESSIONS AND ADD NEW -->
      <value>.*agronavigator.cz*/kalendar.asp\?.*</value>
      <value>.*agroporadenstvi.cz.*/kalendar.asp\?.*</value>
      <value>http://zbraslav\.info/kalendar-akci/.*/201[5-9]/.*</value>
      <value>.*&amp;month:int=[0-9]{1,2}&amp;year:int=(?!(2010|2011)).*</value>
      <value>.*knihy_prehled\.asp\?rok=201[6-9]{1}.*</value>
     </list>
    </entry>
   </map>
  </property>
 </bean>
 <bean class="org.archive.crawler.spring.SurtPrefixesSheetAssociation">
  <property name="surtPrefixes">
   <list>
    <value>+fantasyplanet.cz</value>
    <value>+agroporadenstvi.cz</value>
    <value>+coena.cz</value>
    <value>+agronavigator.cz</value>
    <value>+zbraslav.info</value>
   </list>
  </property>
  <property name="targetSheetNames">
   <list>
    <value>rejectLocalCalendars-sheet</value>
   </list>
  </property>
 </bean>

<!-- LOCAL TRAPS -->
 <bean id="rejectLocalTraps" class="org.archive.modules.deciderules.MatchesListRegexDecideRule" autowire-candidate="false">
  <property name="decision" value="REJECT" />
 </bean>
 <bean id="rejectLocalTraps-sheet" class='org.archive.spring.Sheet'>
  <property name='map'>
   <map>
    <entry key='rejectLocalTraps.regexList'>
     <list>
      <value>.*/profil\.php.*</value>
      <value>.*#comments.*</value>
      <value>.*%E2%8C%A9.*</value>
      <value>.*&amp;do=discussion-.*</value>
      <value>.*&amp;event=inquiry_vote.*</value>
      <value>.*-/17\.4\.5\.1$</value>
      <value>.*-/3\.19\.9\.37$</value>
      <value>.*-/selfbox\.asp$</value>
      <value>.*/.*leta/.*leta/.*</value>
      <value>.*/.*stoleti/.*stoleti/.*</value>
      <value>.*/ClanekHodnoceni/.*</value>
      <value>.*/Shared_UserLoginBar\.asp.*</value>
      <value>.*/[0-9]{4,5}\.[0-9]{1,2}\.[0-9]{1,2}\.[0-9]{1,2}[^/].*</value>
      <value>.*/[0-9]{5}\.[0-9]{1,2}\.[0-9]{1,2}\.[0-9]{1,2}[^/].*</value>
      <value>.*/\?hash=.*</value>
      <value>.*/\?likeit=.*</value>
      <value>.*/\?reply=.*</value>
      <value>.*/_diskuse=reagovat/.*</value>
      <value>.*/action_send/.*</value>
      <value>.*/admin/.*</value>
      <value>.*/antispam\.php\?antispamkod=.*</value>
      <value>.*/bazar/.*</value>
      <value>.*/cgi-bin/mail\.cgi/.*</value>
      <value>.*/clanek-diskuse\.php.*</value>
      <value>.*/clanek-poslat\.php.*</value>
      <value>.*/clanek-tisk\.php.*</value>
      <value>.*/clanek\.asp\?we=diskuze.*</value>
      <value>.*/comment\.php.*</value>
      <value>.*/comment_list\.php.*</value>
      <value>.*/diskuse/pridat.*</value>
      <value>.*/doporucit-stranku/.*</value>
      <value>.*/feed/.*</value>
      <value>.*/forward\?path=node.*</value>
      <value>.*/getProfilUserStats\.php.*</value>
      <value>.*/hlasovat/\?poll.*</value>
      <value>.*/kalendar-akci-na-pamatkach/actions/\?f=&amp;in_f=.*</value>
      <value>.*/news_mail\.php.*</value>
      <value>.*/novy_komentar.*</value>
      <value>.*/outbound-article/.*</value>
      <value>.*/pocasi/.*</value>
      <value>.*/post/1\.5$</value>
      <value>.*/post/quote\.html$</value>
      <value>.*/post/reply\.html$</value>
      <value>.*/print_preview\.php.*</value>
      <value>.*/racebook/.*/.*\?sort=alphabetical</value>
      <value>.*/racebook/.*/.*\?sort=mostactive</value>
      <value>.*/racebook/.*/.*\?sort=mostdiscussed</value>
      <value>.*/racebook/.*/.*\?sort=mostmembers</value>
      <value>.*/racebook/.*/.*\?sort=mostwalls</value>
      <value>.*/register\?destination=comment.*</value>
      <value>.*/search\.php\?query=.*</value>
      <value>.*/search_user\.php.*</value>
      <value>.*/tipafriend/.*</value>
      <value>.*/uimg/vvc/.*</value>
      <value>.*/vlozeni-dotazu\.asp.*</value>
      <value>.*/xml/.*</value>
      <value>.*/zabava-na-mobil/.*</value>
      <value>.*1wM7Wad1eE6Uq5Msxf68HfV4HeHdbGOAcvmCgW520zf\.C7$</value>
      <value>.*=Login$</value>
      <value>.*EditDiscussion.*</value>
      <value>.*EditRequest.*</value>
      <value>.*EditUser.*</value>
      <value>.*Profile.*</value>
      <value>.*\?re=.*</value>
      <value>.*akceKomentar=.*</value>
      <value>.*board-admin\.pl.*</value>
      <value>.*categoryId=.*categoryId=.*</value>
      <value>.*dinsert\.php.*</value>
      <value>.*doporucit\?d=.*</value>
      <value>.*func=showtipresults.*</value>
      <value>.*http://www.radiovaticana.cz/clanek_odeslat.php4\?id=.*</value>
      <value>.*inquiryAnswerId=-1.*</value>
      <value>.*isidorus.net/google/.*</value>
      <value>.*loginr=.*</value>
      <value>.*mebio\.cz/ratings.*</value>
      <value>.*racebook/groups.html\?sort=alphabetical</value>
      <value>.*racebook/groups.html\?sort=mostdiscussed</value>
      <value>.*stranka=1&amp;razeni=.*</value>
      <value>.*view.php3\?vid=966.*</value>
      <value>.*zprava-tisk\.asp\?.*</value>
      <value>^(.*)(/forum/)(.*)$</value>
      <value>^(.*)(login.php)(.*)$</value>
      <value>^(.*)(mode=post)(.*)$</value>
      <value>^(.*)(posting.php)(.*)$</value>
      <value>http://.*http://.*</value>
      <value>http://archiv\.cbvk\.cz/historicke_mapy.*</value>
      <value>http://archiv\.cbvk\.cz/kramerius/.*</value>
     </list>
    </entry>
   </map>
  </property>
 </bean>
 <bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
  <property name="surtPrefixes">
   <list>
    <value>http://(cz,abclinuxu,</value>
    <value>http://(cz,aeoroweb,</value>
    <value>http://(cz,agronavigator,</value>
    <value>http://(cz,agroporadenstvi,</value>
    <value>http://(cz,aperio,</value>
    <value>http://(cz,aquapage,</value>
    <value>http://(cz,asz,</value>
    <value>http://(cz,babyweb,</value>
    <value>http://(cz,beskydy,</value>
    <value>http://(cz,biom,</value>
    <value>http://(cz,ihned,respekt,blog,</value>
    <value>http://(cz,bobosikova,</value>
    <value>http://(cz,wendys,botanika</value>
    <value>http://(cz,cbvk,</value>
    <value>http://(cz,cestr,</value>
    <value>http://(cz,coena,</value>
    <value>http://(org,czechinvest,</value>
    <value>http://(cz,czso,</value>
    <value>http://(cz,divokevino,</value>
    <value>http://(cz,drogy-info,</value>
    <value>http://(cz,e15,</value>
    <value>http://(cz,fantasyplanet,</value>
    <value>http://(cz,flymag,</value>
    <value>http://(cz,hedvabnastezka,</value>
    <value>http://(cz,hkcr,</value>
    <value>http://(cz,iliteratura,</value>
    <value>http://(net,isidorus,</value>
    <value>http://(cz,itpoint,</value>
    <value>http://(cz,izdoprava,</value>
    <value>http://(cz,jvkcb,</value>
    <value>http://(net,k-report,</value>
    <value>http://(cz,khszlin,</value>
    <value>http://(cz,klinikazdravi,</value>
    <value>http://(cz,leadersmagazine,</value>
    <value>http://(cz,marinka,</value>
    <value>http://(cz,mebio,</value>
    <value>http://(cz,mediar,</value>
    <value>http://(cz,mezinami,</value>
    <value>http://(cz,militaria,</value>
    <value>http://(cz,musicserver,</value>
    <value>http://(cz,neaktuality,</value>
    <value>http://(cz,neaktualne,</value>
    <value>http://(cz,lidovky,neviditelnypes,</value>
    <value>http://(cz,nhlpro,</value>
    <value>http://(cz,npu,</value>
    <value>http://(cz,nvf,</value>
    <value>http://(cz,okruhari,</value>
    <value>http://(cz,paraple,</value>
    <value>http://(cz,pravnik,</value>
    <value>http://(info,prohlizece,</value>
    <value>http://(cz,radiovaticana,</value>
    <value>http://(cz,ruce,</value>
    <value>http://(cz,rvp,</value>
    <value>http://(cz,specnaz,</value>
    <value>http://(cz,stavebni-forum,</value>
    <value>http://(cz,valka,</value>
    <value>http://(cz,veda,</value>
    <value>http://(cz,volby,</value>
    <value>http://(com,volleycountry,</value>
    <value>http://(cz,vuzt,</value>
    <value>http://(net,fobiazine,</value>
    <value>http://(com,vlacky,</value>
    <value>http://(info,zbraslav,</value>
    <value>http://(cz,zeny,</value>
   </list>
  </property>
  <property name='targetSheetNames'>
   <list>
    <value>rejectLocalTraps-sheet</value>
   </list>
  </property>
 </bean>


<!-- FACEBOOK Extraction sheet -->
 <bean id="enableFacebookScroll" class="org.archive.spring.Sheet">
  <property name="map">
   <map>
    <entry key="extractorFacebookScroll.enabled" value="true"/>
   </map>
  </property>
 </bean>
 <bean class="org.archive.crawler.spring.SurtPrefixesSheetAssociation">
  <property name="surtPrefixes">
   <list>
    <value>http://(com,facebook,</value>
   </list>
  </property>
  <property name="targetSheetNames">
   <list>
    <value>enableFacebookScroll</value>
   </list>
  </property>
 </bean>

<!-- TWITTER Extraction sheet -->
 <bean id="enableTwitterScrollOne" class="org.archive.spring.Sheet">
  <property name="map">
   <map>
    <entry key="extractorTwitterScrollOne.enabled" value="true"/>
   </map>
  </property>
 </bean>
 <bean class="org.archive.crawler.spring.SurtPrefixesSheetAssociation">
  <property name="surtPrefixes">
   <list>
    <value>http://(com,twitter,)/</value>
    <value>http://(com,twitter,www,)/</value>
   </list>
  </property>
  <property name="targetSheetNames">
   <list>
    <value>enableTwitterScrollOne</value>
   </list>
  </property>
 </bean>
 <bean id="enableTwitterScrollFurther" class="org.archive.spring.Sheet">
  <property name="map">
   <map>
    <entry key="extractorTwitterScrollFurther.enabled" value="true"/>
    <entry key="extractorTwitterScrollOne.enabled" value="false"/>
   </map>
  </property>
 </bean>
 <bean class="org.archive.crawler.spring.SurtPrefixesSheetAssociation">
  <property name="surtPrefixes">
   <list>
    <value>http://(com,twitter,)/i/profiles/show/</value>
    <value>http://(com,twitter,www,)/i/profiles/show/</value>
   </list>
  </property>
  <property name="targetSheetNames">
   <list>
    <value>enableTwitterScrollFurther</value>
   </list>
  </property>
 </bean>

<!-- BIG BUDGET -->
<bean id='bigBudget' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='frontier.queueTotalBudget' value='60000'/>
  </map>
 </property>
</bean>
<bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
 <property name='surtPrefixes'>
  <list>
   <value>+itpoint.cz</value>
   <value>+leadersmagazine.cz</value>
  </list>
 </property>
 <property name='targetSheetNames'>
  <list>
   <value>bigBudget</value>
  </list>
 </property>
</bean>

<!-- GENTLE FETCH -->
<bean id='gentleFetch' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='fetchHttp.maxFetchKBSec' value='30'/>
  </map>
 </property>
</bean>
<bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
 <property name='surtPrefixes'>
  <list>
   <value>+specnaz.cz</value>
   <value>+czso.cz</value>
   <value>+volby.cz</value>
  </list>
 </property>
 <property name='targetSheetNames'>
  <list>
   <value>gentleFetch</value>
  </list>
 </property>
</bean>

<!-- IGNORE COOKIES -->
<bean id='ignoreCookies' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='fetchHttp.ignoreCookies' value='true'/>
  </map>
 </property>
</bean>
<bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
 <property name='surtPrefixes'>
  <list>
   <value>+coena.cz</value>
  </list>
 </property>
 <property name='targetSheetNames'>
  <list>
   <value>ignoreCookies</value>
  </list>
 </property>
</bean>


 <!--
   EXAMPLE SETTINGS OVERLAY SHEETS
   Sheets allow some settings to vary by context - usually by URI context,
   so that different sites or sections of sites can be treated differently. 
   Here are some example Sheets for common purposes. The SheetOverlaysManager
   (below) automatically collects all Sheet instances declared among the 
   original beans, but others can be added during the crawl via the scripting 
   interface.
  -->

<!-- forceRetire: any URI to which this sheet's settings are applied 
     will force its containing queue to 'retired' status. -->
<bean id='forceRetire' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='disposition.forceRetire' value='true'/>
  </map>
 </property>
</bean>

<!-- SMALL BUDGET -->
<!-- smallBudget: any URI to which this sheet's settings are applied 
     will give its containing queue small values for balanceReplenishAmount 
     (causing it to have shorter 'active' periods while other queues are 
     waiting) and queueTotalBudget (causing the queue to enter 'retired' 
     status once that expenditure is reached by URI attempts and errors) -->
<bean id='smallBudget' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='frontier.balanceReplenishAmount' value='1000'/>
   <entry key='frontier.queueTotalBudget' value='5600'/>
  </map>
 </property>
</bean>
<bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
 <property name='surtPrefixes'>
  <list>
   <value>+www.vlacky.com</value>
  </list>
 </property>
 <property name='targetSheetNames'>
  <list>
   <value>smallBudget</value>
  </list>
 </property>
</bean>

<!-- veryPolite: any URI to which this sheet's settings are applied 
     will cause its queue to take extra-long politeness snoozes -->
<bean id='veryPolite' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='disposition.delayFactor' value='10'/>
   <entry key='disposition.minDelayMs' value='10000'/>
   <entry key='disposition.maxDelayMs' value='1000000'/>
   <entry key='disposition.respectCrawlDelayUpToSeconds' value='3600'/>
  </map>
 </property>
</bean>

<!-- highPrecedence: any URI to which this sheet's settings are applied 
     will give its containing queue a slightly-higher than default 
     queue precedence value. That queue will then be preferred over 
     other queues for active crawling, never waiting behind lower-
     precedence queues. -->
<bean id='highPrecedence' class='org.archive.spring.Sheet'>
 <property name='map'>
  <map>
   <entry key='frontier.balanceReplenishAmount' value='20'/>
   <entry key='frontier.queueTotalBudget' value='100'/>
  </map>
 </property>
</bean>

<!--
   EXAMPLE SETTINGS OVERLAY SHEET-ASSOCIATION
   A SheetAssociation says certain URIs should have certain overlay Sheets
   applied. This example applies two sheets to URIs matching two SURT-prefixes.
   New associations may also be added mid-crawl using the scripting facility.
  -->

<!--
<bean class='org.archive.crawler.spring.SurtPrefixesSheetAssociation'>
 <property name='surtPrefixes'>
  <list>
   <value>http://(org,example,</value>
   <value>http://(com,example,www,)/</value>
  </list>
 </property>
 <property name='targetSheetNames'>
  <list>
   <value>veryPolite</value>
   <value>smallBudget</value>
  </list>
 </property>
</bean>
-->

 <!-- 
   OPTIONAL BUT RECOMMENDED BEANS
  -->
  
 <!-- ACTIONDIRECTORY: disk directory for mid-crawl operations
      Running job will watch directory for new files with URIs, 
      scripts, and other data to be processed during a crawl. -->
 <bean id="actionDirectory" class="org.archive.crawler.framework.ActionDirectory">
  <!-- <property name="actionDir" value="action" /> -->
  <!-- <property name="doneDir" value="${launchId}/actions-done" /> -->
  <!-- <property name="initialDelaySeconds" value="10" /> -->
  <!-- <property name="delaySeconds" value="30" /> -->
 </bean> 
 
 <!--  CRAWLLIMITENFORCER: stops crawl when it reaches configured limits -->
 <bean id="crawlLimiter" class="org.archive.crawler.framework.CrawlLimitEnforcer">
  <!-- <property name="maxBytesDownload" value="0" /> -->
  <!-- <property name="maxDocumentsDownload" value="0" /> -->
  <!-- <property name="maxTimeSeconds" value="0" /> -->
 </bean>
 
 <!-- CHECKPOINTSERVICE: checkpointing assistance -->
 <bean id="checkpointService" 
   class="org.archive.crawler.framework.CheckpointService">
  <!-- <property name="checkpointIntervalMinutes" value="-1"/> -->
  <!-- <property name="checkpointsDir" value="checkpoints"/> -->
 </bean>
 
 <!-- 
   OPTIONAL BEANS
    Uncomment and expand as needed, or if non-default alternate 
    implementations are preferred.
  -->
  
 <!-- CANONICALIZATION POLICY -->
 <!--
 <bean id="canonicalizationPolicy" 
   class="org.archive.modules.canonicalize.RulesCanonicalizationPolicy">
   <property name="rules">
    <list>
     <bean class="org.archive.modules.canonicalize.LowercaseRule" />
     <bean class="org.archive.modules.canonicalize.StripUserinfoRule" />
     <bean class="org.archive.modules.canonicalize.StripWWWNRule" />
     <bean class="org.archive.modules.canonicalize.StripSessionIDs" />
     <bean class="org.archive.modules.canonicalize.StripSessionCFIDs" />
     <bean class="org.archive.modules.canonicalize.FixupQueryString" />
    </list>
  </property>
 </bean>
 -->
 

 <!-- QUEUE ASSIGNMENT POLICY -->
 <!--
 <bean id="queueAssignmentPolicy" 
   class="org.archive.crawler.frontier.SurtAuthorityQueueAssignmentPolicy">
  <property name="forceQueueAssignment" value="" />
  <property name="deferToPrevious" value="true" />
  <property name="parallelQueues" value="1" />
 </bean>
 -->
 
 <!-- URI PRECEDENCE POLICY -->
 <!--
 <bean id="uriPrecedencePolicy" 
   class="org.archive.crawler.frontier.precedence.CostUriPrecedencePolicy">
 </bean>
 -->
 
 <!-- COST ASSIGNMENT POLICY -->
 <!--
 <bean id="costAssignmentPolicy" 
   class="org.archive.crawler.frontier.UnitCostAssignmentPolicy">
 </bean>
 -->
 
 <!-- CREDENTIAL STORE: HTTP authentication or FORM POST credentials -->
 <!-- 
 <bean id="credentialStore" 
   class="org.archive.modules.credential.CredentialStore">
 </bean>
 -->
 
 <!-- DISK SPACE MONITOR: 
      Pauses the crawl if disk space at monitored paths falls below minimum threshold -->
 
 <bean id="diskSpaceMonitor" class="org.archive.crawler.monitor.DiskSpaceMonitor">
   <property name="pauseThresholdMiB" value="5000" />
   <property name="monitorConfigPaths" value="true" />
   <property name="monitorPaths">
     <list>
       <value>/opt/heritrix/jobs</value>
     </list>
   </property>
 </bean>
 
 <!-- 
   REQUIRED STANDARD BEANS
    It will be very rare to replace or reconfigure the following beans.
  -->

 <!-- STATISTICSTRACKER: standard stats/reporting collector -->
 <bean id="statisticsTracker" 
   class="org.archive.crawler.reporting.StatisticsTracker" autowire="byName">
  <!-- <property name="reports">
        <list>
         <bean id="crawlSummaryReport" class="org.archive.crawler.reporting.CrawlSummaryReport" />
         <bean id="seedsReport" class="org.archive.crawler.reporting.SeedsReport" />
         <bean id="hostsReport" class="org.archive.crawler.reporting.HostsReport" />
         <bean id="sourceTagsReport" class="org.archive.crawler.reporting.SourceTagsReport" />
         <bean id="mimetypesReport" class="org.archive.crawler.reporting.MimetypesReport" />
         <bean id="responseCodeReport" class="org.archive.crawler.reporting.ResponseCodeReport" />
         <bean id="processorsReport" class="org.archive.crawler.reporting.ProcessorsReport" />
         <bean id="frontierSummaryReport" class="org.archive.crawler.reporting.FrontierSummaryReport" />
         <bean id="frontierNonemptyReport" class="org.archive.crawler.reporting.FrontierNonemptyReport" />
         <bean id="toeThreadsReport" class="org.archive.crawler.reporting.ToeThreadsReport" />
        </list>
       </property> -->
  <!-- <property name="reportsDir" value="${launchId}/reports" /> -->
  <!-- <property name="liveHostReportSize" value="20" /> -->
  <!-- <property name="intervalSeconds" value="20" /> -->
  <!-- <property name="keepSnapshotsCount" value="5" /> -->
  <!-- <property name="liveHostReportSize" value="20" /> -->
 </bean>
 
 <!-- CRAWLERLOGGERMODULE: shared logging facility -->
 <bean id="loggerModule" 
   class="org.archive.crawler.reporting.CrawlerLoggerModule">
  <!-- <property name="path" value="${launchId}/logs" /> -->
  <!-- <property name="crawlLogPath" value="crawl.log" /> -->
  <!-- <property name="alertsLogPath" value="alerts.log" /> -->
  <!-- <property name="progressLogPath" value="progress-statistics.log" /> -->
  <!-- <property name="uriErrorsLogPath" value="uri-errors.log" /> -->
  <!-- <property name="runtimeErrorsLogPath" value="runtime-errors.log" /> -->
  <!-- <property name="nonfatalErrorsLogPath" value="nonfatal-errors.log" /> -->
  <!-- <property name="logExtraInfo" value="true" /> -->
 </bean>
 
 <!-- SHEETOVERLAYMANAGER: manager of sheets of contextual overlays
      Autowired to include any SheetForSurtPrefix or 
      SheetForDecideRuled beans -->
 <bean id="sheetOverlaysManager" autowire="byType"
   class="org.archive.crawler.spring.SheetOverlaysManager">
 </bean>

 <!-- BDBMODULE: shared BDB-JE disk persistence manager -->
 <bean id="bdb" 
  class="org.archive.bdb.BdbModule">
  <property name="dir" value="[see override above]" />
  <!-- if neither cachePercent or cacheSize are specified (the default), bdb
       uses its own default of 60% -->
  <!-- <property name="cachePercent" value="0" /> -->
  <!-- <property name="cacheSize" value="0" /> -->
  <!-- <property name="useSharedCache" value="true" /> -->
  <!-- <property name="expectedConcurrency" value="25" /> -->
 </bean>

 <!-- URL-Agnostic Duplication Reduction -->
 <bean id="historyBdb" class="org.archive.bdb.BdbModule" autowire-candidate="false">
  <property name="dir" value="[see override above]" />
 </bean>
  
 <bean id="contentDigestHistory" class="org.archive.modules.recrawl.BdbContentDigestHistory">
  <property name="bdbModule">
   <ref bean="historyBdb" />
  </property>
 </bean>
 <!-- EOF URL-Agnostic Duplication Reduction -->


 
 <!-- BDBCOOKIESTORAGE: disk-based cookie storage for FetchHTTP -->
 <bean id="cookieStorage" 
   class="org.archive.modules.fetcher.BdbCookieStorage">
  <!-- <property name="cookiesLoadFile"><null/></property> -->
  <!-- <property name="cookiesSaveFile"><null/></property> -->
  <!-- <property name="bdb">
        <ref bean="bdb"/>
       </property> -->
 </bean>
 
 <!-- SERVERCACHE: shared cache of server/host info -->
 <bean id="serverCache" 
   class="org.archive.modules.net.BdbServerCache">
  <!-- <property name="bdb">
        <ref bean="bdb"/>
       </property> -->
 </bean>

 <!-- CONFIG PATH CONFIGURER: required helper making crawl paths relative
      to crawler-beans.cxml file, and tracking crawl files for web UI -->
 <bean id="configPathConfigurer" 
   class="org.archive.spring.ConfigPathConfigurer">
 </bean>
 
</beans>
